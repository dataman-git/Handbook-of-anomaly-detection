{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f12ec631-0d57-4a6d-a0ec-7940abac6968",
   "metadata": {},
   "source": [
    "## 12. Autoencoders\n",
    "\n",
    "- Remember to use tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9021294f-5f48-4f53-8b36-773852531a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.396090</td>\n",
       "      <td>2.092611</td>\n",
       "      <td>2.073392</td>\n",
       "      <td>1.988262</td>\n",
       "      <td>1.953473</td>\n",
       "      <td>2.450997</td>\n",
       "      <td>1.631040</td>\n",
       "      <td>1.746182</td>\n",
       "      <td>1.898050</td>\n",
       "      <td>2.380148</td>\n",
       "      <td>...</td>\n",
       "      <td>1.703454</td>\n",
       "      <td>2.502966</td>\n",
       "      <td>2.119108</td>\n",
       "      <td>2.106098</td>\n",
       "      <td>2.165173</td>\n",
       "      <td>2.340826</td>\n",
       "      <td>2.170109</td>\n",
       "      <td>1.749139</td>\n",
       "      <td>1.678661</td>\n",
       "      <td>1.829647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.775596</td>\n",
       "      <td>1.829438</td>\n",
       "      <td>2.054768</td>\n",
       "      <td>1.577190</td>\n",
       "      <td>1.594549</td>\n",
       "      <td>1.373357</td>\n",
       "      <td>1.946647</td>\n",
       "      <td>1.841420</td>\n",
       "      <td>1.595761</td>\n",
       "      <td>2.538094</td>\n",
       "      <td>...</td>\n",
       "      <td>1.974274</td>\n",
       "      <td>1.621608</td>\n",
       "      <td>2.003085</td>\n",
       "      <td>2.076871</td>\n",
       "      <td>1.788868</td>\n",
       "      <td>2.062829</td>\n",
       "      <td>2.084499</td>\n",
       "      <td>2.267568</td>\n",
       "      <td>1.536939</td>\n",
       "      <td>2.132725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.835679</td>\n",
       "      <td>1.612100</td>\n",
       "      <td>2.174908</td>\n",
       "      <td>2.084460</td>\n",
       "      <td>2.472896</td>\n",
       "      <td>2.029110</td>\n",
       "      <td>2.410107</td>\n",
       "      <td>2.282164</td>\n",
       "      <td>2.208201</td>\n",
       "      <td>2.106240</td>\n",
       "      <td>...</td>\n",
       "      <td>2.035652</td>\n",
       "      <td>2.065291</td>\n",
       "      <td>2.197711</td>\n",
       "      <td>2.288806</td>\n",
       "      <td>2.480274</td>\n",
       "      <td>1.946207</td>\n",
       "      <td>1.947120</td>\n",
       "      <td>1.754344</td>\n",
       "      <td>2.265033</td>\n",
       "      <td>2.119050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.420241</td>\n",
       "      <td>2.158485</td>\n",
       "      <td>1.958602</td>\n",
       "      <td>1.903787</td>\n",
       "      <td>2.230522</td>\n",
       "      <td>1.984789</td>\n",
       "      <td>1.964441</td>\n",
       "      <td>2.360795</td>\n",
       "      <td>1.820773</td>\n",
       "      <td>2.116560</td>\n",
       "      <td>...</td>\n",
       "      <td>2.040977</td>\n",
       "      <td>1.511381</td>\n",
       "      <td>1.834332</td>\n",
       "      <td>2.070046</td>\n",
       "      <td>1.911699</td>\n",
       "      <td>1.816916</td>\n",
       "      <td>2.213950</td>\n",
       "      <td>2.099758</td>\n",
       "      <td>2.259999</td>\n",
       "      <td>2.039066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.646926</td>\n",
       "      <td>1.778067</td>\n",
       "      <td>1.760959</td>\n",
       "      <td>1.894087</td>\n",
       "      <td>1.888225</td>\n",
       "      <td>2.228021</td>\n",
       "      <td>2.489542</td>\n",
       "      <td>2.326377</td>\n",
       "      <td>1.969615</td>\n",
       "      <td>2.001316</td>\n",
       "      <td>...</td>\n",
       "      <td>2.063858</td>\n",
       "      <td>2.341009</td>\n",
       "      <td>1.844115</td>\n",
       "      <td>2.076399</td>\n",
       "      <td>1.742857</td>\n",
       "      <td>1.969530</td>\n",
       "      <td>1.821128</td>\n",
       "      <td>1.946249</td>\n",
       "      <td>1.678283</td>\n",
       "      <td>1.797722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  2.396090  2.092611  2.073392  1.988262  1.953473  2.450997  1.631040   \n",
       "1  1.775596  1.829438  2.054768  1.577190  1.594549  1.373357  1.946647   \n",
       "2  1.835679  1.612100  2.174908  2.084460  2.472896  2.029110  2.410107   \n",
       "3  2.420241  2.158485  1.958602  1.903787  2.230522  1.984789  1.964441   \n",
       "4  1.646926  1.778067  1.760959  1.894087  1.888225  2.228021  2.489542   \n",
       "\n",
       "         7         8         9   ...        15        16        17        18  \\\n",
       "0  1.746182  1.898050  2.380148  ...  1.703454  2.502966  2.119108  2.106098   \n",
       "1  1.841420  1.595761  2.538094  ...  1.974274  1.621608  2.003085  2.076871   \n",
       "2  2.282164  2.208201  2.106240  ...  2.035652  2.065291  2.197711  2.288806   \n",
       "3  2.360795  1.820773  2.116560  ...  2.040977  1.511381  1.834332  2.070046   \n",
       "4  2.326377  1.969615  2.001316  ...  2.063858  2.341009  1.844115  2.076399   \n",
       "\n",
       "         19        20        21        22        23        24  \n",
       "0  2.165173  2.340826  2.170109  1.749139  1.678661  1.829647  \n",
       "1  1.788868  2.062829  2.084499  2.267568  1.536939  2.132725  \n",
       "2  2.480274  1.946207  1.947120  1.754344  2.265033  2.119050  \n",
       "3  1.911699  1.816916  2.213950  2.099758  2.259999  2.039066  \n",
       "4  1.742857  1.969530  1.821128  1.946249  1.678283  1.797722  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyod.utils.data import generate_data\n",
    "contamination = 0.05 # percentage of outliers\n",
    "n_train = 500       # number of training points\n",
    "n_test = 500        # number of testing points\n",
    "n_features = 25      # number of features\n",
    "X_train, X_test, y_train, y_test = generate_data(\n",
    "    n_train=n_train, \n",
    "    n_test=n_test, \n",
    "    n_features= n_features, \n",
    "    contamination=contamination, \n",
    "    random_state=123)\n",
    "\n",
    "X_train_pd = pd.DataFrame(X_train)\n",
    "X_train_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "704a1157-7b92-442d-94d3-fe83a8c25d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# User-defined utility functions #\n",
    "##################################\n",
    "\n",
    "def count_stat(vector):\n",
    "    # Because it is '0' and '1', \n",
    "    # we can run a count statistic. \n",
    "    unique, counts = np.unique(vector, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "def descriptive_stat_threshold(df, pred_score, threshold):\n",
    "    # Let's see how many '0's and '1's.\n",
    "    df = pd.DataFrame(df)\n",
    "    df['Anomaly_Score'] = pred_score\n",
    "    df['Group'] = np.where(df['Anomaly_Score']< threshold, \n",
    "                 'Normal', 'Outlier')\n",
    "    # Now let's show the summary statistics:\n",
    "    cnt = df.groupby('Group')['Anomaly_Score'].count().reset_index().rename(columns={'Anomaly_Score':'Count'})\n",
    "    # The count and count %\n",
    "    cnt['Count %'] = (cnt['Count'] / cnt['Count'].sum()) * 100 \n",
    "    # The average\n",
    "    stat = df.groupby('Group').mean().round(2).reset_index()\n",
    "    # Put the count and the average together\n",
    "    stat = cnt.merge(stat, left_on='Group', right_on='Group') \n",
    "    return (stat)\n",
    "\n",
    "def confusion_matrix_threshold(\n",
    "    actual,score, threshold):\n",
    "    Actual_pred = pd.DataFrame({'Actual': actual, 'Pred': score})\n",
    "    Actual_pred['Pred'] = np.where(Actual_pred['Pred']<=threshold,0,1)\n",
    "    cm = pd.crosstab(Actual_pred['Actual'],Actual_pred['Pred'])\n",
    "    return (cm)\n",
    "\n",
    "def confusion_matrix(actual,pred):\n",
    "    Actual_pred = pd.DataFrame({'Actual': actual,'Pred': pred})\n",
    "    cm = pd.crosstab(Actual_pred['Actual'],Actual_pred['Pred'])\n",
    "    return (cm)\n",
    "\n",
    "def plot_data():\n",
    "    plt.scatter(X_train_pd[0], X_train_pd[1],c=y_train, alpha=0.8)\n",
    "    plt.title('Scatter plot')\n",
    "    plt.xlabel('x0')\n",
    "    plt.ylabel('x1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05bcc7a0-bafc-425e-af0a-e48a584cbf24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 400x240 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(5, 3), dpi=80)\n",
    "plt.scatter(X_train_pd[0], X_train_pd[1], c=y_train, alpha=0.8)\n",
    "plt.title('Scatter plot')\n",
    "plt.xlabel('x0')\n",
    "plt.ylabel('x1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453e735-4c51-4ab3-93e0-a1b83fd15a42",
   "metadata": {},
   "source": [
    "### Step 1: Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea5b38f5-3d40-43b2-a465-1810e5a0b363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 18:20:59.086569: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-22 18:20:59.087760: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 52        \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 25)                75        \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 3.8550 - val_loss: 1.9628\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 3.2131 - val_loss: 1.7831\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 2.8343 - val_loss: 1.6570\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 2.6043 - val_loss: 1.5792\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 2.4779 - val_loss: 1.5176\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 2.3505 - val_loss: 1.4663\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 2.2633 - val_loss: 1.4220\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 2.1913 - val_loss: 1.3782\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 117us/sample - loss: 2.1199 - val_loss: 1.3434\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 2.0750 - val_loss: 1.3101\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 2.0268 - val_loss: 1.2776\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.9829 - val_loss: 1.2462\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 117us/sample - loss: 1.9431 - val_loss: 1.2151\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.9043 - val_loss: 1.1840\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.8639 - val_loss: 1.1531\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.8285 - val_loss: 1.1232\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.7883 - val_loss: 1.0936\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.7605 - val_loss: 1.0656\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.7154 - val_loss: 1.0350\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.6879 - val_loss: 1.0078\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.6659 - val_loss: 0.9842\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.6385 - val_loss: 0.9624\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.6001 - val_loss: 0.9413\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.6004 - val_loss: 0.9224\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.5650 - val_loss: 0.9049\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.5463 - val_loss: 0.8872\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.5364 - val_loss: 0.8720\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.5017 - val_loss: 0.8579\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.4944 - val_loss: 0.8440\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.4793 - val_loss: 0.8315\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.4596 - val_loss: 0.8183\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.4540 - val_loss: 0.8072\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.4425 - val_loss: 0.7973\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.4298 - val_loss: 0.7881\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.4267 - val_loss: 0.7792\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.4118 - val_loss: 0.7709\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.4007 - val_loss: 0.7629\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.3841 - val_loss: 0.7549\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.3838 - val_loss: 0.7479\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 128us/sample - loss: 1.3689 - val_loss: 0.7412\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.3767 - val_loss: 0.7348\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.3591 - val_loss: 0.7284\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 117us/sample - loss: 1.3596 - val_loss: 0.7221\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.3459 - val_loss: 0.7163\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.3374 - val_loss: 0.7111\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 117us/sample - loss: 1.3361 - val_loss: 0.7062\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.3317 - val_loss: 0.7011\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.3121 - val_loss: 0.6963\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3175 - val_loss: 0.6919\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.3202 - val_loss: 0.6876\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.3192 - val_loss: 0.6834\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2938 - val_loss: 0.6791\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2993 - val_loss: 0.6753\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2909 - val_loss: 0.6716\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2938 - val_loss: 0.6680\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2909 - val_loss: 0.6645\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2856 - val_loss: 0.6611\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.2774 - val_loss: 0.6577\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2582 - val_loss: 0.6545\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.2628 - val_loss: 0.6512\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2710 - val_loss: 0.6487\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2666 - val_loss: 0.6457\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.2519 - val_loss: 0.6423\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2626 - val_loss: 0.6403\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.2560 - val_loss: 0.6378\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2570 - val_loss: 0.6350\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.2436 - val_loss: 0.6321\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2526 - val_loss: 0.6300\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.2593 - val_loss: 0.6276\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2560 - val_loss: 0.6256\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2404 - val_loss: 0.6263\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2261 - val_loss: 0.6223\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.2324 - val_loss: 0.6189\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2317 - val_loss: 0.6161\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2328 - val_loss: 0.6143\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2334 - val_loss: 0.6120\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2167 - val_loss: 0.6099\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.2218 - val_loss: 0.6085\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2181 - val_loss: 0.6069\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.2185 - val_loss: 0.6068\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2137 - val_loss: 0.6037\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2121 - val_loss: 0.6022\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2120 - val_loss: 0.5999\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.2198 - val_loss: 0.5980\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.1989 - val_loss: 0.5964\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2050 - val_loss: 0.5952\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2002 - val_loss: 0.5947\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 117us/sample - loss: 1.2011 - val_loss: 0.5920\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.1887 - val_loss: 0.5904\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.1945 - val_loss: 0.5886\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.1979 - val_loss: 0.5864\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.1890 - val_loss: 0.5850\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1977 - val_loss: 0.5838\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.1875 - val_loss: 0.5829\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.1854 - val_loss: 0.5829\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.1919 - val_loss: 0.5809\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.1849 - val_loss: 0.5797\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.1803 - val_loss: 0.5779\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.1857 - val_loss: 0.5768\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.1896 - val_loss: 0.5753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoEncoder(batch_size=32, contamination=0.05, dropout_rate=0.2, epochs=100,\n",
       "      hidden_activation='relu', hidden_neurons=[2, 2], l2_regularizer=0.1,\n",
       "      loss=<function mean_squared_error at 0x7fba44b40a70>,\n",
       "      optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "      random_state=None, validation_size=0.1, verbose=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "atcdr = AutoEncoder(contamination=0.05, hidden_neurons =[2, 2])\n",
    "atcdr.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec6fa50-0f8b-4ef3-bb1f-581aa69158bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The threshold for the defined contamination rate: 4.129931070161943\n",
      "The training data: {0: 475, 1: 25}\n",
      "The training data: {0: 475, 1: 25}\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "y_train_scores = atcdr.decision_function(X_train)\n",
    "y_train_pred = atcdr.predict(X_train)\n",
    "\n",
    "# Test data\n",
    "y_test_scores = atcdr.decision_function(X_test)\n",
    "y_test_pred = atcdr.predict(X_test) # outlier labels (0 or 1)\n",
    "\n",
    "# Threshold for the defined comtanimation rate\n",
    "print(\"The threshold for the defined contamination rate:\" , atcdr.threshold_)\n",
    "\n",
    "def count_stat(vector):\n",
    "    # Because it is '0' and '1', we can run a count statistic. \n",
    "    unique, counts = np.unique(vector, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "print(\"The training data:\", count_stat(y_train_pred))\n",
    "print(\"The training data:\", count_stat(y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a150d79-9337-4890-ae82-bb312d166a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'contamination': 0.05,\n",
       " 'dropout_rate': 0.2,\n",
       " 'epochs': 100,\n",
       " 'hidden_activation': 'relu',\n",
       " 'hidden_neurons': [2, 2],\n",
       " 'l2_regularizer': 0.1,\n",
       " 'loss': <function tensorflow.python.keras.losses.mean_squared_error(y_true, y_pred)>,\n",
       " 'optimizer': 'adam',\n",
       " 'output_activation': 'sigmoid',\n",
       " 'preprocessing': True,\n",
       " 'random_state': None,\n",
       " 'validation_size': 0.1,\n",
       " 'verbose': 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atcdr.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1943f6f-327d-497b-9a4a-5ef69864ddc1",
   "metadata": {},
   "source": [
    "### Step 2: Determine the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9015e76e-d382-4bfb-867f-96eccef1d5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train_scores, bins='auto') # arguments are passed to np.histogram\n",
    "plt.title(\"Outlier score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33284af5-5a22-465a-ad83-78bb81e3cd9e",
   "metadata": {},
   "source": [
    "### Step 3: Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b6f4a1-7215-4874-a32a-450a70ff1af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>20.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5    6  ...  \\\n",
       "0   Normal    475     95.0  2.01  2.01  2.02  2.00  2.00  2.00  2.0  ...   \n",
       "1  Outlier     25      5.0 -0.02  0.21 -0.05  0.13  0.01 -0.32  0.2  ...   \n",
       "\n",
       "     16    17    18    19    20    21   22    23    24  Anomaly_Score  \n",
       "0  1.99  2.02  2.01  2.00  1.99  2.00  2.0  2.01  1.99           2.11  \n",
       "1 -0.07 -0.30  0.11  0.18  0.14 -0.28  0.2  0.09 -0.10          20.25  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = atcdr.threshold_ # Or other value from the above histogram\n",
    "\n",
    "def descriptive_stat_threshold(df,pred_score, threshold):\n",
    "    # Let's see how many '0's and '1's.\n",
    "    df = pd.DataFrame(df)\n",
    "    df['Anomaly_Score'] = pred_score\n",
    "    df['Group'] = np.where(df['Anomaly_Score']< threshold, 'Normal', 'Outlier')\n",
    "\n",
    "    # Now let's show the summary statistics:\n",
    "    cnt = df.groupby('Group')['Anomaly_Score'].count().reset_index().rename(columns={'Anomaly_Score':'Count'})\n",
    "    cnt['Count %'] = (cnt['Count'] / cnt['Count'].sum()) * 100 # The count and count %\n",
    "    stat = df.groupby('Group').mean().round(2).reset_index() # The avg.\n",
    "    stat = cnt.merge(stat, left_on='Group',right_on='Group') # Put the count and the avg. together\n",
    "    return (stat)\n",
    "\n",
    "descriptive_stat_threshold(X_train,y_train_scores, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9a42470-3753-4067-a35d-9832310d5379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    475     95.0  2.00  1.98  2.02  2.01  1.99  2.01  1.99  ...   \n",
       "1  Outlier     25      5.0 -0.22 -0.45  0.04 -0.26 -0.34 -0.14  0.12  ...   \n",
       "\n",
       "     16   17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  2.00  2.0  2.01  2.00  2.00  1.99  2.01  1.98  1.99           2.10  \n",
       "1 -0.07 -0.1  0.15  0.02 -0.36  0.12  0.50 -0.08  0.36          20.85  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_test,y_test_scores, atcdr.threshold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de5405d9-690b-43ad-a4a1-6c32156ac9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     475   0\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc91fcbc-aa09-4c08-b0c7-d77310ccc904",
   "metadata": {},
   "source": [
    "### Step 4: Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73d2bb66-b6ef-44fc-9c9f-0dbfb4b484a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 52        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 6         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 25)                75        \n",
      "=================================================================\n",
      "Total params: 1,433\n",
      "Trainable params: 1,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 4.2697 - val_loss: 1.1434\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 3.6187 - val_loss: 1.1021\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 3.1842 - val_loss: 1.0685\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 2.9603 - val_loss: 1.0404\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 2.7683 - val_loss: 1.0147\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 2.6306 - val_loss: 0.9920\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 2.5256 - val_loss: 0.9691\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 116us/sample - loss: 2.4058 - val_loss: 0.9483\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 117us/sample - loss: 2.3097 - val_loss: 0.9273\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 2.2323 - val_loss: 0.9071\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 116us/sample - loss: 2.1653 - val_loss: 0.8869\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 116us/sample - loss: 2.1141 - val_loss: 0.8663\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 117us/sample - loss: 2.0618 - val_loss: 0.8453\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 117us/sample - loss: 2.0170 - val_loss: 0.8241\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 117us/sample - loss: 1.9733 - val_loss: 0.8029\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.9395 - val_loss: 0.7819\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.9075 - val_loss: 0.7615\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 139us/sample - loss: 1.8639 - val_loss: 0.7410\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.8422 - val_loss: 0.7210\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.8144 - val_loss: 0.7018\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.7795 - val_loss: 0.6837\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.7585 - val_loss: 0.6657\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.7212 - val_loss: 0.6482\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.7041 - val_loss: 0.6315\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.6929 - val_loss: 0.6161\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.6648 - val_loss: 0.6008\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.6335 - val_loss: 0.5857\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 123us/sample - loss: 1.6309 - val_loss: 0.5718\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.6090 - val_loss: 0.5587\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.6000 - val_loss: 0.5461\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.5848 - val_loss: 0.5348\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.5609 - val_loss: 0.5241\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 166us/sample - loss: 1.5539 - val_loss: 0.5137\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.5338 - val_loss: 0.5035\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.5251 - val_loss: 0.4937\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.5197 - val_loss: 0.4846\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.5080 - val_loss: 0.4763\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.4909 - val_loss: 0.4680\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.4827 - val_loss: 0.4603\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 128us/sample - loss: 1.4767 - val_loss: 0.4529\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.4496 - val_loss: 0.4457\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.4547 - val_loss: 0.4390\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.4546 - val_loss: 0.4326\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.4342 - val_loss: 0.4264\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.4395 - val_loss: 0.4204\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.4268 - val_loss: 0.4148\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.4206 - val_loss: 0.4093\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.4195 - val_loss: 0.4043\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.4173 - val_loss: 0.3995\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3973 - val_loss: 0.3949\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.3869 - val_loss: 0.3902\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.3979 - val_loss: 0.3859\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3797 - val_loss: 0.3817\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.3751 - val_loss: 0.3776\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.3715 - val_loss: 0.3736\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3551 - val_loss: 0.3697\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3541 - val_loss: 0.3661\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3645 - val_loss: 0.3627\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.3626 - val_loss: 0.3594\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.3580 - val_loss: 0.3560\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.3414 - val_loss: 0.3529\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.3398 - val_loss: 0.3498\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.3403 - val_loss: 0.3469\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 130us/sample - loss: 1.3328 - val_loss: 0.3442\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.3325 - val_loss: 0.3415\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.3204 - val_loss: 0.3389\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.3061 - val_loss: 0.3363\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 126us/sample - loss: 1.3148 - val_loss: 0.3338\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 128us/sample - loss: 1.3239 - val_loss: 0.3314\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.3330 - val_loss: 0.3291\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 125us/sample - loss: 1.3181 - val_loss: 0.3268\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.3148 - val_loss: 0.3246\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.3115 - val_loss: 0.3224\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.3094 - val_loss: 0.3203\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.2985 - val_loss: 0.3183\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2987 - val_loss: 0.3164\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 121us/sample - loss: 1.2796 - val_loss: 0.3145\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2930 - val_loss: 0.3126\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2834 - val_loss: 0.3107\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 127us/sample - loss: 1.2837 - val_loss: 0.3090\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 120us/sample - loss: 1.2954 - val_loss: 0.3073\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 124us/sample - loss: 1.2709 - val_loss: 0.3056\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2571 - val_loss: 0.3039\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2777 - val_loss: 0.3023\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 119us/sample - loss: 1.2756 - val_loss: 0.3007\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 118us/sample - loss: 1.2813 - val_loss: 0.2992\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 122us/sample - loss: 1.2629 - val_loss: 0.2977\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.2628 - val_loss: 0.2962\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.2656 - val_loss: 0.2948\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 130us/sample - loss: 1.2589 - val_loss: 0.2934\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 412us/sample - loss: 1.2602 - val_loss: 0.2921\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 351us/sample - loss: 1.2564 - val_loss: 0.2907\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 1.2443 - val_loss: 0.2895\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 1.2562 - val_loss: 0.2882\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 179us/sample - loss: 1.2453 - val_loss: 0.2869\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 212us/sample - loss: 1.2465 - val_loss: 0.2857\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 1.2491 - val_loss: 0.2845\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 1.2389 - val_loss: 0.2834\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 1.2401 - val_loss: 0.2822\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 1.2528 - val_loss: 0.2810\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                260       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 25)                275       \n",
      "=================================================================\n",
      "Total params: 1,887\n",
      "Trainable params: 1,887\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 3.2933 - val_loss: 3.1910\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 2.7815 - val_loss: 2.8464\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 167us/sample - loss: 2.5233 - val_loss: 2.6354\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 2.3482 - val_loss: 2.4858\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 2.2332 - val_loss: 2.3729\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 2.1396 - val_loss: 2.2793\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 2.0571 - val_loss: 2.1949\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.9852 - val_loss: 2.1239\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 130us/sample - loss: 1.9192 - val_loss: 2.0498\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.8620 - val_loss: 1.9820\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.7976 - val_loss: 1.9162\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 136us/sample - loss: 1.7444 - val_loss: 1.8518\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.6952 - val_loss: 1.7957\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.6472 - val_loss: 1.7432\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.6011 - val_loss: 1.6983\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.5590 - val_loss: 1.6583\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.5362 - val_loss: 1.6219\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.4950 - val_loss: 1.5912\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.4711 - val_loss: 1.5616\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.4429 - val_loss: 1.5339\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 146us/sample - loss: 1.4268 - val_loss: 1.5118\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 259us/sample - loss: 1.3955 - val_loss: 1.4911\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 496us/sample - loss: 1.3822 - val_loss: 1.4712\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 316us/sample - loss: 1.3661 - val_loss: 1.4533\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 296us/sample - loss: 1.3512 - val_loss: 1.4356\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 1.3330 - val_loss: 1.4179\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 276us/sample - loss: 1.3146 - val_loss: 1.4023\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 1.3036 - val_loss: 1.3874\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 1.2999 - val_loss: 1.3741\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.2842 - val_loss: 1.3611\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 212us/sample - loss: 1.2731 - val_loss: 1.3509\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.2605 - val_loss: 1.3396\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 143us/sample - loss: 1.2528 - val_loss: 1.3310\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.2477 - val_loss: 1.3204\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.2407 - val_loss: 1.3111\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.2254 - val_loss: 1.3028\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 139us/sample - loss: 1.2188 - val_loss: 1.2952\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.2083 - val_loss: 1.2893\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.2105 - val_loss: 1.2825\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.2027 - val_loss: 1.2758\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.1997 - val_loss: 1.2693\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.1867 - val_loss: 1.2638\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 139us/sample - loss: 1.1792 - val_loss: 1.2582\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 137us/sample - loss: 1.1851 - val_loss: 1.2530\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 141us/sample - loss: 1.1772 - val_loss: 1.2473\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 138us/sample - loss: 1.1709 - val_loss: 1.2428\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 144us/sample - loss: 1.1687 - val_loss: 1.2388\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 266us/sample - loss: 1.1675 - val_loss: 1.2334\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 1.1590 - val_loss: 1.2287\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 141us/sample - loss: 1.1617 - val_loss: 1.2241\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 1.1514 - val_loss: 1.2200\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 249us/sample - loss: 1.1549 - val_loss: 1.2170\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 1.1480 - val_loss: 1.2131\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.1464 - val_loss: 1.2093\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 1.1446 - val_loss: 1.2057\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 164us/sample - loss: 1.1420 - val_loss: 1.2033\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 171us/sample - loss: 1.1394 - val_loss: 1.1992\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 1.1351 - val_loss: 1.1967\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 181us/sample - loss: 1.1315 - val_loss: 1.1923\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 161us/sample - loss: 1.1305 - val_loss: 1.1891\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 182us/sample - loss: 1.1254 - val_loss: 1.1866\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 180us/sample - loss: 1.1295 - val_loss: 1.1834\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 1.1256 - val_loss: 1.1822\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 1.1247 - val_loss: 1.1802\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 1.1211 - val_loss: 1.1780\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 162us/sample - loss: 1.1206 - val_loss: 1.1754\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.1194 - val_loss: 1.1737\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.1146 - val_loss: 1.1704\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1162 - val_loss: 1.1671\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 1.1124 - val_loss: 1.1681\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.1095 - val_loss: 1.1656\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.1067 - val_loss: 1.1645\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.1046 - val_loss: 1.1620\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 1.1061 - val_loss: 1.1600\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 206us/sample - loss: 1.1080 - val_loss: 1.1599\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 207us/sample - loss: 1.1072 - val_loss: 1.1609\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 171us/sample - loss: 1.1057 - val_loss: 1.1603\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 1.1009 - val_loss: 1.1582\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 252us/sample - loss: 1.0964 - val_loss: 1.1569\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 1.0924 - val_loss: 1.1537\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.0932 - val_loss: 1.1552\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.0922 - val_loss: 1.1534\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.1005 - val_loss: 1.1495\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 176us/sample - loss: 1.0917 - val_loss: 1.1469\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 178us/sample - loss: 1.0869 - val_loss: 1.1476\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 214us/sample - loss: 1.0910 - val_loss: 1.1465\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.0916 - val_loss: 1.1417\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 129us/sample - loss: 1.0852 - val_loss: 1.1404\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 133us/sample - loss: 1.0841 - val_loss: 1.1398\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 129us/sample - loss: 1.0812 - val_loss: 1.1413\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 142us/sample - loss: 1.0866 - val_loss: 1.1399\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 1.0820 - val_loss: 1.1384\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 1.0726 - val_loss: 1.1379\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 168us/sample - loss: 1.0779 - val_loss: 1.1395\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.0833 - val_loss: 1.1381\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.0814 - val_loss: 1.1364\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 131us/sample - loss: 1.0809 - val_loss: 1.1352\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 134us/sample - loss: 1.0787 - val_loss: 1.1363\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 135us/sample - loss: 1.0717 - val_loss: 1.1351\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 132us/sample - loss: 1.0711 - val_loss: 1.1334\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 25)                650       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 15)                390       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                160       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 22        \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 15)                165       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 25)                400       \n",
      "=================================================================\n",
      "Total params: 2,467\n",
      "Trainable params: 2,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 450 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 2s 3ms/sample - loss: 4.4662 - val_loss: 1.8895\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 176us/sample - loss: 3.5197 - val_loss: 1.7160\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 160us/sample - loss: 3.0304 - val_loss: 1.5907\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 2.6780 - val_loss: 1.4970\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 2.4831 - val_loss: 1.4228\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 2.3162 - val_loss: 1.3470\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 2.1747 - val_loss: 1.2709\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 2.0599 - val_loss: 1.1973\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 242us/sample - loss: 1.9601 - val_loss: 1.1249\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 1.8613 - val_loss: 1.0599\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 165us/sample - loss: 1.7821 - val_loss: 1.0021\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.7163 - val_loss: 0.9560\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.6676 - val_loss: 0.9213\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.6165 - val_loss: 0.8923\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.5843 - val_loss: 0.8673\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.5487 - val_loss: 0.8452\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.5236 - val_loss: 0.8255\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.4963 - val_loss: 0.8075\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.4817 - val_loss: 0.7916\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 1.4538 - val_loss: 0.7778\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 1.4368 - val_loss: 0.7648\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 206us/sample - loss: 1.4165 - val_loss: 0.7527\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.3981 - val_loss: 0.7415\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.3843 - val_loss: 0.7313\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.3796 - val_loss: 0.7218\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.3664 - val_loss: 0.7133\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 162us/sample - loss: 1.3503 - val_loss: 0.7046\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.3445 - val_loss: 0.6966\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.3347 - val_loss: 0.6897\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.3222 - val_loss: 0.6829\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.3181 - val_loss: 0.6762\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.3067 - val_loss: 0.6701\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.3002 - val_loss: 0.6666\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.2957 - val_loss: 0.6592\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.2869 - val_loss: 0.6538\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.2822 - val_loss: 0.6492\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.2751 - val_loss: 0.6447\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.2682 - val_loss: 0.6422\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.2648 - val_loss: 0.6367\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.2572 - val_loss: 0.6317\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.2550 - val_loss: 0.6275\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.2455 - val_loss: 0.6235\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.2416 - val_loss: 0.6197\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 181us/sample - loss: 1.2411 - val_loss: 0.6164\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 1.2345 - val_loss: 0.6144\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 1.2293 - val_loss: 0.6111\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 177us/sample - loss: 1.2293 - val_loss: 0.6081\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.2222 - val_loss: 0.6042\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.2195 - val_loss: 0.6014\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.2158 - val_loss: 0.5978\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 159us/sample - loss: 1.2136 - val_loss: 0.5953\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.2076 - val_loss: 0.5926\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.2055 - val_loss: 0.5904\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.2053 - val_loss: 0.5880\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.2026 - val_loss: 0.5858\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1946 - val_loss: 0.5837\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1958 - val_loss: 0.5816\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1944 - val_loss: 0.5798\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.1924 - val_loss: 0.5782\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 1.1901 - val_loss: 0.5765\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 1.1904 - val_loss: 0.5744\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 203us/sample - loss: 1.1858 - val_loss: 0.5722\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1841 - val_loss: 0.5709\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.1785 - val_loss: 0.5689\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1835 - val_loss: 0.5669\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1812 - val_loss: 0.5654\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 153us/sample - loss: 1.1753 - val_loss: 0.5638\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 157us/sample - loss: 1.1723 - val_loss: 0.5626\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1708 - val_loss: 0.5617\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1702 - val_loss: 0.5602\n",
      "Epoch 71/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1708 - val_loss: 0.5606\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1737 - val_loss: 0.5574\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1737 - val_loss: 0.5555\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1593 - val_loss: 0.5546\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1642 - val_loss: 0.5526\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 148us/sample - loss: 1.1598 - val_loss: 0.5539\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1600 - val_loss: 0.5506\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.1568 - val_loss: 0.5489\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.1586 - val_loss: 0.5475\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1538 - val_loss: 0.5460\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 158us/sample - loss: 1.1522 - val_loss: 0.5449\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1582 - val_loss: 0.5433\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.1525 - val_loss: 0.5425\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 149us/sample - loss: 1.1513 - val_loss: 0.5422\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.1653 - val_loss: 0.5466\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 155us/sample - loss: 1.1609 - val_loss: 0.5411\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 156us/sample - loss: 1.1506 - val_loss: 0.5384\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 147us/sample - loss: 1.1472 - val_loss: 0.5372\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.1444 - val_loss: 0.5357\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 169us/sample - loss: 1.1482 - val_loss: 0.5363\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 167us/sample - loss: 1.1396 - val_loss: 0.5346\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 162us/sample - loss: 1.1451 - val_loss: 0.5339\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.1403 - val_loss: 0.5326\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.1376 - val_loss: 0.5311\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 152us/sample - loss: 1.1411 - val_loss: 0.5305\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1383 - val_loss: 0.5306\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 154us/sample - loss: 1.1367 - val_loss: 0.5300\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1342 - val_loss: 0.5305\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 151us/sample - loss: 1.1334 - val_loss: 0.5286\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 150us/sample - loss: 1.1339 - val_loss: 0.5277\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.combination import aom, moa, average, maximization\n",
    "from pyod.utils.utility import standardizer\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "atcdr1 = AutoEncoder(contamination=0.05, hidden_neurons =[2, 2])\n",
    "atcdr2 = AutoEncoder(contamination=0.05, hidden_neurons =[10, 2, 10])\n",
    "atcdr3 = AutoEncoder(contamination=0.05, hidden_neurons =[15, 10, 2, 10, 15] )\n",
    "\n",
    "# Standardize data\n",
    "X_train_norm, X_test_norm = standardizer(X_train, X_test)\n",
    "\n",
    "# Just prepare data frames so we can store the model results. There are three models.\n",
    "train_scores = np.zeros([X_train.shape[0], 3])\n",
    "test_scores = np.zeros([X_test.shape[0], 3])\n",
    "atcdr1.fit(X_train_norm)\n",
    "atcdr2.fit(X_train_norm)\n",
    "atcdr3.fit(X_train_norm)\n",
    "    \n",
    "# Store the results in each column:\n",
    "train_scores[:, 0] = atcdr1.decision_function(X_train_norm) \n",
    "train_scores[:, 1] = atcdr2.decision_function(X_train_norm) \n",
    "train_scores[:, 2] = atcdr3.decision_function(X_train_norm)\n",
    "test_scores[:, 0] = atcdr1.decision_function(X_test_norm) \n",
    "test_scores[:, 1] = atcdr2.decision_function(X_test_norm) \n",
    "test_scores[:, 2] = atcdr3.decision_function(X_test_norm)\n",
    "\n",
    "# Decision scores have to be normalized before combination\n",
    "train_scores_norm, test_scores_norm = standardizer(train_scores,test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ec9923e-c4df-432e-96dd-e92e5b66497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 4), dpi=80)\n",
    "# Combination by average\n",
    "y_train_by_average = average(train_scores_norm)\n",
    "y_test_by_average = average(test_scores_norm)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_train_by_average, bins='auto') # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by average\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37c2b2e2-ebe3-490c-8008-c9984613a60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>474</td>\n",
       "      <td>94.8</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>26</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    474     94.8  2.01  2.01  2.03  2.00  2.00  2.00  2.00  ...   \n",
       "1  Outlier     26      5.2  0.05  0.30  0.01  0.18  0.09 -0.24  0.28  ...   \n",
       "\n",
       "     16    17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  1.99  2.02  2.01  2.00  1.99  2.00  2.00  2.01  1.99          -0.23  \n",
       "1  0.00 -0.21  0.20  0.25  0.21 -0.18  0.27  0.18 -0.03           4.15  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_train,y_train_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6a23053-4d1a-4d3e-992a-416439aa5ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Count</th>\n",
       "      <th>Count %</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>Anomaly_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Normal</td>\n",
       "      <td>475</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.98</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.99</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1.98</td>\n",
       "      <td>1.99</td>\n",
       "      <td>-0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Outlier</td>\n",
       "      <td>25</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.36</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>4.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group  Count  Count %     0     1     2     3     4     5     6  ...  \\\n",
       "0   Normal    475     95.0  2.00  1.98  2.02  2.01  1.99  2.01  1.99  ...   \n",
       "1  Outlier     25      5.0 -0.22 -0.45  0.04 -0.26 -0.34 -0.14  0.12  ...   \n",
       "\n",
       "     16   17    18    19    20    21    22    23    24  Anomaly_Score  \n",
       "0  2.00  2.0  2.01  2.00  2.00  1.99  2.01  1.98  1.99          -0.23  \n",
       "1 -0.07 -0.1  0.15  0.02 -0.36  0.12  0.50 -0.08  0.36           4.47  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptive_stat_threshold(X_test,y_test_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19589ca4-8802-4014-9d14-3c378f31c39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>474</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     474   1\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,y_train_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f86490-1377-4b2c-8fad-21c622a33e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Pred</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>475</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Pred      0   1\n",
       "Actual         \n",
       "0.0     475   0\n",
       "1.0       0  25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_test_by_average, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196290e-37dd-4328-a264-25b0d1b75436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b1996-c86d-4192-b5f5-57a1235c2fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fef3d4-4594-485f-86d8-e8e6538d7f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
